# Râ€‘VFiDÂ (Semanticâ€“Frequencyâ€“Prompt)Â ImplementationÂ Guide

> **Goal**Â â€”Â Reâ€‘implement the full Râ€‘VFiD pipeline in Python/PyTorch so it can be trained, evaluated and extended for AAAIâ€¯2026 submission.

---

## âœ…Â ProgressÂ Checklist

> Tick every box when the subâ€‘task is done.  
> **Bold** = mandatory for MVP, ðŸ”„ = iterative refinement.

### 0Â Environment & dependencies

### 1Â Data loader & augmentation

### 2Â Zeroâ€‘shot CLIP topâ€‘_c_ classifier

### 3Â Routerâ€‘Prompt pool builder

### 4Â Text encoder forward & prompt fineâ€‘tuning

### 5Â Vision backbone (frozen ViTâ€‘B/16)

### 6Â LoRA frequencyâ€‘expert injection

### 7Â Crossâ€‘attention router Î±â€‘weights

### 8Â Fusion (concatÂ +Â SE) & classifier head

### 9Â Losses

### 10Â Training loop & scheduler

### 11Â Inference singleâ€‘pass ensemble

### 12Â Continualâ€‘learning update

### 13Â Logging, metrics & visualisation

### 14Â Checkpointing & modelâ€‘card

---

## 1Â SystemÂ Overview

Râ€‘VFiD fuses **highâ€‘level semantics, lowâ€‘level frequency cues and promptâ€‘based PEFT**. The computation graph (batchÂ size _B_, featureÂ dim _d_â€¯=â€¯768):

```
 xÂ âˆˆÂ BÂ Ã—Â 3Â Ã—Â 224Â Ã—Â 224 â”€â–¶Â E_vÂ (ViTâ€‘B/16, frozen) â”€â–¶ V_tokÂ (BÂ Ã—Â TÂ Ã—Â d)
                                      â”‚
                                      â”œâ”€â–ºÂ K LoRA expertsÂ {F_iÂ âˆˆÂ BÂ Ã—Â d}
                                      â”‚
 Routerâ€‘Prompt poolÂ P_rÂ (BÂ Ã—Â L'Â Ã—Â d) â”€â”€â”¤
                                      â–¼
                         Crossâ€‘Attention Router â”€â–¶ Î±Â (BÂ Ã—Â K)
                                      â–¼
                V_freqÂ =Â Î£Â Î±_iÂ F_i Â (BÂ Ã—Â d)
                                      â–¼
     V_fuseÂ =Â SE([V_clsÂ âˆ¥Â V_freq]) Â (BÂ Ã—Â 2d)
                                      â–¼
                LoRAÂ HeadÂ â†’Â logitsÂ (BÂ Ã—Â 2)
```

Parameter economy:Â Î”Î¸Â â‰ˆÂ **2â€“3â€¯%** of CLIP.

---

## 2Â Moduleâ€‘byâ€‘Module Specification

|Stage|Purpose|InputÂ shape|OutputÂ shape|TrainableÂ params|
|---|---|---|---|---|
|0Â Preâ€‘proc|RGBÂ â†’ CLIPÂ normalisedÂ tensor|BÃ—3Ã—HÃ—W|BÃ—3Ã—224Ã—224|â€”|
|1Â Zeroâ€‘shot topâ€‘_c_Â labels|Imageâ€‘conditioned semantics|BÃ—3Ã—224Ã—224|list(lenâ€¯=â€¯c)|â€”|
|2Â Routerâ€‘Prompt pool|Build _L'Â =Â 2cÂ·7_ tokens|list|BÃ—L'Ã—d|âœ” (tokenÂ emb)|
|3Â TextÂ EncoderÂ E_t|map tokensâ†’embs|BÃ—L'Ã—d|BÃ—L'Ã—d|frozen|
|4Â Vision backboneÂ E_v|semantic tokens|BÃ—3Ã—224Ã—224|V_tokÂ (BÃ—TÃ—d)|frozen|
|5Â LoRA expertÂ _i_|lowâ€‘level cue _Ci_|V_tok|F_iÂ (BÃ—d)|âœ” (râ€¯=â€¯4,Â Î±â€¯=â€¯8)|
|6Â RouterÂ Xâ€‘Attn|Î±â€‘weights|P_r,Â V_tok|Î±Â (BÃ—K)|âœ”|
|7Â Freq sum|cue aggregation|Î±,{F_i}|V_freqÂ (BÃ—d)|â€”|
|8Â ConcatÂ +Â SE|fuse hi/lo|V_cls,V_freq|V_fuseÂ (BÃ—2d)|âœ”(tiny)|
|9Â LoRAÂ Head|binary logits|V_fuse|BÃ—2|âœ”|

### 2.1Â RouterÂ Prompt (readâ€‘only)

- **LengthÂ LÂ =â€¯7** tokens per sentence, duplicated for _2c_ real/fake templates.Â 
    
- Updated **only** at embedding level; CLIP weights untouched.
    

### 2.2Â LoRA Frequency Experts

- Injected into **Q,K,V** of every ViT block:  
    `W_qkvÂ â†Â W_qkvÂ +Â Î±/rÂ Â·Â BÂ A` (rank _r_â€¯=â€¯4).
    
- Three recommended cues: **NPR**, **DnCNN residual**, **NoisePrint**.
    

### 2.3Â Crossâ€‘Attention Router

- `QÂ =Â P_rÂ W_q ,Â KÂ =Â V_tokÂ W_k ,Â VÂ =Â V_tokÂ W_v` â†’Â `Î±Â =Â softmax(QKáµ€/âˆšd)`.
    
- Singleâ€‘head sufficient; output size _(B,K)_.
    

### 2.4Â Fusion & Head

- `V_catÂ =Â [V_clsÂ âˆ¥Â V_freq]` (_BÃ—2d_).
    
- Squeezeâ€‘andâ€‘Excite: `sÂ =Â Ïƒ(Wâ‚‚Â Î´(Wâ‚Â Avg(V_cat))) ;Â V_fuseÂ =Â sÂ âŠ™Â V_cat`.
    
- Head: LoRAâ€‘adapted linear â†’ logits.
    

---

## 3Â Loss Functions

|   |   |   |
|---|---|---|
|Loss|Formula|Weight|
|BinaryÂ CE|âˆ’[yÂ logâ€¯pÂ +Â (1âˆ’y)Â logâ€¯(1âˆ’p)]|1.0|
|InfoNCE|sim(P_r,Â V_sem)Â +Â sim(P_r,Â V_freq)|0.1|
|GatingÂ Entropy|âˆ’Î£â€¯Î±â€¯logâ€¯Î±|0.01|

Total:Â `LÂ =Â L_CEÂ +Â 0.1Â L_NCEÂ +Â 0.01Â L_ent`.

---

## 4Â TrainingÂ Pipeline

1. **Load** frozen CLIP (ViTâ€‘B/16) weights.
    
2. Build **Dataset & DataLoader** with onâ€‘theâ€‘fly lowâ€‘level extraction.
    
3. **Forward pass** per Sec.â€¯2; accumulate losses.
    
4. **Optimizer**: AdamW, lrÂ 1eâ€‘3, weightâ€‘decayÂ 0.01, cosineÂ anneal.
    
5. **Epochs**:Â 20Â per generator domain.
    
6. **Validation** on AIGCDetect; logÂ Acc,Â AUC,Â AP.
    

---

## 5Â Inference (singleâ€‘pass ensemble)

```
with torch.no_grad():
    logits = model(x, prompt_pool_all_domains)  # shape BÃ—2
    p_fake = torch.sigmoid(logits[:,1])
```

- If domainâ€‘prob classifier (kâ€‘means on CLS) is desired, scale task scores before argmax.
    

---

## 6Â Continualâ€‘Learning Update

1. **Freeze** all existing prompts & experts.
    
2. **Add** new readâ€‘only promptÂ _p_k+1_ and (optionally) new LoRA expert.
    
3. **Finetune** only these additions on new generator data (â‰¤5Â epochs).
    
4. **Deploy**: still one forward pass, as prompts are independent.Â 
    

---

## 7Â Key Hyperâ€‘parameters

|   |   |   |
|---|---|---|
|Name|Value|Note|
|ImageÂ size|224Â²|CLIP default|
|PatchÂ T|197|1Â CLSÂ +Â 14Ã—14|
|PromptÂ topâ€‘c|5|zeroâ€‘shot classes|
|PromptÂ lenÂ L|7|tokens per sentence|
|LoRA rankÂ r|4|experts & head|
|LoRA Î±|8|scalingÂ factor|
|K experts|3|NPR, DnCNN, NoisePrint|
|BatchÂ size|64|Fit 24â€¯GB GPU|
|BaseÂ lr|1eâ€‘3|AdamW|

---

## 8Â Reference Implementations & Assets

- Prompt2GuardÂ (officialÂ PyTorch) â–¶Â GitHubÂ `laitifranz/Prompt2Guard`Â 
    
- ALEI (Adaptive Lowâ€‘level Experts Injection) â–¶Â GitHubÂ linkÂ TBDÂ 
    

Use these repos for data loaders and lowâ€‘level cue extraction utilities.

---

## 9Â AppendixÂ A â€” Complete Tensor I/O Flow

|   |   |   |
|---|---|---|
|Stage|Tensor symbol|Shape (B= batch, d=768)|
|Preâ€‘proc|x|BÂ Ã—Â 3Â Ã—Â 224Â Ã—Â 224|
|E_v â†“|V_tok|BÂ Ã—Â TÂ Ã—Â d|
|LoRA_i â†“|F_i|BÂ Ã—Â d|
|P_r|â€”|BÂ Ã—Â L'Â Ã—Â d|
|Router|Î±|BÂ Ã—Â K|
|Sum|V_freq|BÂ Ã—Â d|
|Concat|V_cat|BÂ Ã—Â 2d|
|SE|V_fuse|BÂ Ã—Â 2d|
|Head|logits|BÂ Ã—Â 2|

> **Tip**Â â€”Â Run `torchsummary` or `fvcore.nn.FlopCountAnalysis` to verify parameter &Â FLOPs budgets.

---

Happy coding & good luck with the AAAIÂ 2026 deadline!Â ðŸŽ¯