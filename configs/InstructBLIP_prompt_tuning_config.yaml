# configs/InstructBLIP_prompt_tuning_config.yaml

# 資料設定 (Data Settings)
data:
  train_path: "/raid/dannyliu/dataset/GAI_Dataset/Chameleon/test"
  val_test_path: "/raid/dannyliu/dataset/GAI_Dataset/Chameleon/test"
  num_train_samples: 10000                    # 訓練樣本數
  num_val_samples: 1000                      # 驗證樣本數
  num_test_samples: 1000
  seed: 42

# 模型設定 (Model Settings)
model:
  name_pretrained: "Salesforce/instructblip-vicuna-7b"
  finetune_method: "prompt_tuning"
  prompt_tuning_params:
    num_virtual_tokens: 20                  # 虛擬token數量，通常在10-100之間
    init_method: "RANDOM"                   # 初始化方法: "RANDOM" 或 "TEXT"
    init_text: null                         # 如果使用TEXT初始化，可以提供初始文本
    tokenizer_name_or_path: null           # 將在代碼中自動設置

# 訓練設定 (Training Settings)
training:
  should_train: true
  epochs: 2000                                 # 減少epoch數以避免過擬合
  batch_size: 128                            # 減小batch size以避免記憶體問題
  eval_batch_size: 128                        # 減小評估batch size
  learning_rate_full: 5.0e-5
  learning_rate_prompt_tuning: 1.0e-2      # 調整學習率
  warmup_steps: 150                          # 減少warmup steps
  weight_decay: 0.0                         # Prompt tuning通常不使用weight decay
  max_target_token_length: 16
  prompt_config_idx: 0
  evaluation_strategy: "steps"
  eval_steps: 50                            # 調整評估頻率
  save_strategy: "steps"
  save_steps: 50                            # 調整保存頻率
  save_total_limit: 5                       # 減少保存的檢查點數量
  early_stopping_patience: 50               # 調整早停耐心值
  early_stopping_threshold: 0.001
  gradient_accumulation_steps: 2            # 增加梯度累積來模擬更大的batch size
  use_bfloat16: true
  gradient_checkpointing: true              # 啟用梯度檢查點以節省記憶體
  logging_steps: 10                         # 調整日誌頻率

# 輸出設定 (Output Settings)
output:
  base_results_dir_root: "results/instructblip_prompt_tuning_chameleon"

# 環境設定 (Environment Settings)
environment:
  gpu_id: 2
  disable_wandb: true